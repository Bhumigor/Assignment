{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b0d40b-194e-44ba-b597-a934d049390b",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57574ce-be34-492e-b715-3cb25144c738",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to transform numeric features in a dataset into a specific range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the difference between the maximum and minimum values. This process scales the values proportionally, preserving the relative relationships between the data points.\n",
    "\n",
    "Mathematically, Min-Max scaling for a feature x is done as follows:\n",
    "x scale = (x - x min)/(x max - x min)\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset containing the age of people:\n",
    "Age = [25,30,40,20,50]\n",
    "\n",
    "Age_Scale = {(25-20)/(50-20)} , {(30-20)/(50-20)} , {(40-20)/(50-20)} , {(20-20)/(50-20)} , {(50-20)/(50-20)}\n",
    "         \n",
    "          = {(5/30) , (10/30) , (20/30) , (0/30) , (30/30)}\n",
    "          \n",
    "          = {0.166,0.333,0.666,0,1}\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f75fe9-1ed1-4e48-8521-6bf1c08d3842",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ea64b-83bf-471b-a264-49e25637292a",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling, also known as normalization, is used to scale the feature values in a dataset to have unit norm. It means that each data point's feature vector is scaled down to have a magnitude of 1 while preserving its direction. This technique is useful when the magnitude of the features is not important, but the direction is significant.\n",
    "\n",
    "Mathematically, the Unit Vector scaling for a feature vector x is done as follows:\n",
    "x scale = x / ∥x∥\n",
    "\n",
    "Example:\n",
    "Consider a dataset containing two features, \"height\" and \"weight\":\n",
    "height = [160,170,155]\n",
    "weight = [60,70,50]\n",
    "\n",
    "x1 scale = 160/(sqrt(160^2 + 60^2)) = 160/170.880 = 0.936\n",
    "\n",
    "x2 scale = 60/(sqrt(160^2 + 60^2)) = 60/170.880 = 0.351\n",
    "\n",
    "x3 scale = 170/(sqrt(170^2 + 70^2)) = 170/183.848 = 0.924\n",
    "\n",
    "x4 scale = 70/(sqrt(170^2 + 70^2)) = 70/183.848 = 0.381\n",
    "\n",
    "x5 scale = 155/(sqrt(155^2 + 50^2)) = 155/162.865 = 0.952\n",
    "\n",
    "x6 scale = 50/(sqrt(155^2 + 50^2)) = 50/162.865 = 0.307"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a631a5-9614-4fd7-9ac0-90f99cc029de",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb0499-db8c-487c-bba7-26ab1922385c",
   "metadata": {},
   "source": [
    " PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining most of the relevant information. It achieves this by finding new orthogonal (uncorrelated) axes called principal components, where the variance along these components is maximized. The first principal component captures the most significant variance, the second captures the second most significant, and so on.\n",
    " \n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "1. Standardize the data (mean centering and scaling).\n",
    "2. Compute the covariance matrix of the standardized data.\n",
    "3. Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "4. Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "5. Choose the top k eigenvectors (principal components) to reduce the dimensionality (where k < original feature dimensions).\n",
    "6. Project the data onto the selected principal components.\n",
    "\n",
    "Example:\n",
    "Let's say we have a 2-dimensional dataset with points (1, 3), (2, 4), (3, 5), (4, 4), and (5, 6). We want to reduce the dimensionality to 1 using PCA.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d44cf-5900-44c1-9718-609936d43adc",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e588bf-4bc2-4a78-8070-06f1fc3233fa",
   "metadata": {},
   "source": [
    "PCA and Feature Extraction are related concepts in the context of dimensionality reduction. PCA is a specific technique for performing Feature Extraction. Feature Extraction is the process of transforming the original features of a dataset into a new set of features that represent the most important patterns or variations in the data. PCA is one of the most popular methods for Feature Extraction.\n",
    "\n",
    "PCA extracts a set of orthogonal features, called principal components, that capture the maximum variance in the data. The first principal component represents the direction along which the data varies the most, the second principal component represents the second most significant direction, and so on. By choosing a subset of these principal components, we can reduce the dimensionality of the data while retaining a significant amount of the original information.\n",
    "\n",
    "example:\n",
    "We have a 2-dimensional dataset with points (1, 3), (2, 4), (3, 5), (4, 4), and (5, 6). We want to reduce the dimensionality to 1 using PCA.\n",
    "\n",
    "Step 1: Standardize the data (mean centering and scaling)\n",
    "The mean of the data: mean_x = (1 + 2 + 3 + 4 + 5)/5 = 3\n",
    "The standard deviation of the data: std_x = sqrt((1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2)/5 = 0.632\n",
    "\n",
    "Standardized data:\n",
    "x1 = (1-3)/0.632 = -3.165\n",
    "\n",
    "x2 = (2-3)/0.632 = -1.582\n",
    "\n",
    "x3 = (3-3)/0.632 = 0\n",
    "\n",
    "x4 = (4-3)/0.632 = 1.582\n",
    "\n",
    "x5 = (5-3)/0.632 = 3.165\n",
    "\n",
    "Step 2: Compute the covariance matrix of the standardized data.\n",
    "\n",
    "Step 3: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "The eigenvalues (\\lambda) and The projected data will be a 1-dimensional array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7aec0-6f84-4f0a-a301-2a107c4c396e",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52cadd-25c1-4039-b43b-17035542db37",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data, especially if the features have different scales and ranges. By applying Min-Max scaling, we can ensure that all features are transformed into a common range, typically between 0 and 1, making them comparable and eliminating the impact of different scales.\n",
    "\n",
    "For instance, suppose we have a dataset with features like \"price,\" \"rating,\" and \"delivery time.\" The ranges of these features are quite different. \n",
    "\n",
    "For example:\n",
    "- Price: Range from $5 to $50\n",
    "- Rating: Range from 1 to 5\n",
    "- Delivery Time: Range from 10 minutes to 60 minutes\n",
    "\n",
    "By applying Min-Max scaling, we can transform these features into a common range between 0 and 1, like this:\n",
    "- Price: Scaled from $5-$50 to 0-1\n",
    "- Rating: Scaled from 1-5 to 0-1\n",
    "- Delivery Time: Scaled from 10-60 minutes to 0-1\n",
    "\n",
    "This way, the recommendation system will treat each feature equally when making recommendations, avoiding any bias due to different scales. It ensures that the features contribute uniformly to the recommendation process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dadb90-d3c9-4c9f-9cc5-46f5d620a81c",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b14ccb-010a-44a8-9143-d2f1dffd1aeb",
   "metadata": {},
   "source": [
    "In the context of predicting stock prices, PCA can be used to reduce the dimensionality of the dataset, which often contains numerous features like company financial data and market trends. The high dimensionality of the dataset can lead to increased computational complexity and potential overfitting.\n",
    "\n",
    "By applying PCA, we can identify the principal components that explain most of the variance in the data. These principal components represent the most important patterns or trends in the stock market data. By selecting a smaller number of principal components, we can reduce the dimensionality of the dataset while preserving a significant portion of the original information.\n",
    "\n",
    "The steps to use PCA for dimensionality reduction in the stock price prediction project are :\n",
    "1. Standardize the data: This involves mean centering and scaling each feature to have zero mean and unit variance.\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data.\n",
    "3. Calculate eigenvectors and eigenvalues: Determine the eigenvectors and corresponding eigenvalues of the covariance matrix.\n",
    "4. Sort eigenvectors: Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "5. Choose the top k eigenvectors: Select the first k eigenvectors that explain most of the variance in the data.\n",
    "6. Project the data: Transform the original dataset by projecting it onto the selected k eigenvectors.\n",
    "\n",
    "By choosing a smaller value of k (number of principal components) compared to the original feature dimensions, we effectively reduce the dimensionality of the dataset. The reduced dataset with k principal components can then be used as input for training a stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11d509-e7f1-4c49-abfd-fcce59a410b3",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689fc43a-cff1-4707-9477-650f9495e5f3",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, we follow these steps:\n",
    "1. Find the minimum and maximum values in the dataset:\n",
    "    * Minimum value (min_x) = 1\n",
    "    * Maximum value (max_x) = 20\n",
    "2. Apply the Min-Max scaling formula to each value in the dataset:\n",
    "    * x scale = {{2*(x - x min)}/{(x max - x min)}} - 1\n",
    "    * x1 = 2*(1 - 1)/(20 - 1) - 1 = -1\n",
    "    * x2 = 2*(5 - 1)/(20 - 1) - 1 = -0.578\n",
    "    * x3 = 2*(10 - 1)/(20 - 1) - 1 = 0.052\n",
    "    * x4 = 2*(15 - 1)/(20 - 1) - 1 = 0.473\n",
    "    * x5 = 2*(20 - 1)/(20 - 1) - 1 = 1\n",
    "    \n",
    "Now, the correct Min-Max scaled dataset is approximately [-1, -0.578, 0.052, 0.473, 1]. These values are now within the range of -1 to 1 after Min-Max scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8929e88-cff7-402a-be6a-e74010309f8f",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d494e8-ca57-4242-a174-c2f3a0551efc",
   "metadata": {},
   "source": [
    "For the dataset containing features [height, weight, age, gender, blood pressure], we can use PCA for feature extraction and dimensionality reduction. The goal is to select a smaller number of principal components to represent the data while retaining as much relevant information as possible.\n",
    "\n",
    "The number of principal components to retain depends on the desired level of explained variance. The explained variance represents the amount of information captured by each principal component. To choose the number of principal components to retain, we can analyze the explained variance ratio.\n",
    "\n",
    "Here's the process to decide the number of principal components:\n",
    "1. Standardize the data: Mean center and scale each feature to have zero mean and unit variance.\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data.\n",
    "3. Calculate eigenvectors and eigenvalues: Determine the eigenvectors and corresponding eigenvalues of the covariance matrix.\n",
    "4. Sort eigenvectors: Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "5. Calculate explained variance ratio: Divide each eigenvalue by the sum of all eigenvalues to get the explained variance ratio for each principal component.\n",
    "6. Choose the number of principal components: Decide on the number of principal components to retain based on the explained variance ratio.\n",
    "\n",
    "For example, if the explained variance ratio shows that the first two principal components explain 90% of the variance, we may choose to retain only these two components, resulting in a 2-dimensional reduced dataset.\n",
    "\n",
    "Keep in mind that the number of principal components to retain is a trade-off between reducing dimensionality and retaining enough information for accurate modeling. It is essential to balance complexity reduction with model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d1518-5fbd-491e-90d4-5c1b19d7846b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
