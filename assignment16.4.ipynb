{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a789ddb-0e3c-4582-9152-63d3e594c93b",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21069f-3833-4155-b830-34dfa2caf51c",
   "metadata": {},
   "source": [
    "A1. Polynomial functions and kernel functions are related in the context of Support Vector Machines (SVMs) and other machine learning algorithms, particularly when dealing with non-linear data. Kernel functions are used to implicitly map data from the input space to a higher-dimensional feature space, making it possible to find a linear decision boundary in that higher-dimensional space. Polynomial kernel functions are a specific type of kernel function used for this purpose.\n",
    "\n",
    "The relationship can be summarized as follows:\n",
    "\n",
    "+ A polynomial kernel is a type of kernel function that calculates the dot product between two vectors in a higher-dimensional space without explicitly mapping the data to that space.\n",
    "+ Polynomial kernels are often used when the decision boundary in the original input space is non-linear and can be better represented in a higher-dimensional space.\n",
    "+ The polynomial kernel function computes the dot product using a polynomial function of the original features, allowing the SVM to capture non-linear relationships between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09b8aa-f93f-4432-83d6-c29ca9a1efbc",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8a017e-27ef-476e-9ccf-3a3c24a4f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (e.g., the Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (e.g., scale features)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "\n",
    "classifier = SVC(kernel='poly', degree=3)  \n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9de9ad-1781-45ee-bdd7-fd32f5401141",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5d985-9ab5-40eb-b1da-1046764ea9d4",
   "metadata": {},
   "source": [
    "A3. In Support Vector Regression (SVR), epsilon (ε) is a hyperparameter that determines the width of the epsilon-insensitive tube around the regression line. This tube defines a margin within which errors are not penalized. The choice of epsilon influences the number of support vectors and the trade-off between model complexity and fitting to the training data.\n",
    "\n",
    "- Increasing epsilon makes the epsilon-insensitive tube wider, allowing more data points to be within the tube without contributing to the margin violation loss.\n",
    "- As epsilon increases, the SVR model becomes more tolerant of errors and may accept a larger number of support vectors.\n",
    "- A larger epsilon may result in a simpler model with more support vectors, which can be beneficial when dealing with noisy data or when you want to avoid overfitting.\n",
    "- Conversely, a smaller epsilon creates a narrower tube, leading to a more strict margin. This can result in a model with fewer support vectors and potentially better generalization to the test data.\n",
    "- The choice of epsilon depends on the specific problem and the trade-off between model complexity and fitting to the training data. It should be selected through cross-validation or other hyperparameter tuning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98247b6-7721-473a-bc99-b2ec479e4c54",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985efe5-5b8b-4638-ba8d-2fbc73295164",
   "metadata": {},
   "source": [
    "A4. \n",
    "\n",
    "Kernel Function: The choice of kernel function in SVR determines how the data is transformed into a higher-dimensional space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel should be based on the data's characteristics. For example, if the data has a non-linear relationship, an RBF or polynomial kernel may be appropriate.\n",
    "\n",
    "C Parameter: The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A smaller C leads to a wider margin but allows more training errors, while a larger C results in a narrower margin but fewer errors.\n",
    "- Increase C: Use a larger C when you want to reduce training errors and have a stricter margin. This can lead to a more complex model.\n",
    "- Decrease C: Use a smaller C when you want a wider margin and are willing to tolerate some training errors. This can lead to a simpler model.\n",
    "\n",
    "Epsilon Parameter (ε): The epsilon parameter determines the width of the epsilon-insensitive tube around the regression line. Data points within this tube are not penalized in the loss function.\n",
    "- Increase ε: Use a larger ε when you want the SVR model to be more tolerant of errors and potentially have more support vectors. This can be useful for noisy data.\n",
    "- Decrease ε: Use a smaller ε when you want the model to be less tolerant of errors and have fewer support vectors. This can lead to a stricter fit.\n",
    "\n",
    "Gamma Parameter (for RBF Kernel): The gamma parameter defines the influence of individual training samples on the model. A smaller gamma makes the influence broader, while a larger gamma makes it narrower.\n",
    "- Increase gamma: Use a larger gamma when you suspect that the data has intricate local patterns, and you want the model to focus on specific data points. This can lead to a more complex model.\n",
    "- Decrease gamma: Use a smaller gamma when you want the influence of data points to be more widespread. This can lead to a smoother model with broader generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c496335-b6dc-486c-a56a-af9d3530df18",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataseg\n",
    "- Split the dataset into training and testing setZ\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "- Create an instance of the SVC classifier and train it on the training datW\n",
    "- hse the trained classifier to predict the labels of the testing datW\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,recision, recall, F1-scoreK\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performanc_\n",
    "- Train the tuned classifier on the entire dataseg\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60248b71-7abf-44dd-88b9-b715b3bc6597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Best Hyperparameters: {'C': 1, 'gamma': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib  # For saving the trained classifier\n",
    "\n",
    "# Load a dataset \n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier with an RBF kernel\n",
    "classifier = SVC(kernel='rbf', C=1.0)  \n",
    "\n",
    "# Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV \n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_classifier = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'])\n",
    "tuned_classifier.fit(X, y)  \n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_classifier, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f1c26-fbf4-48ca-814d-d7da7f58d37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
