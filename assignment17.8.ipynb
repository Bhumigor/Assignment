{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4699d6a-f571-442e-881b-67c2472e2a3b",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9f6f5-313d-47eb-91bd-901447c16167",
   "metadata": {},
   "source": [
    "A1. Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble learning method that builds a predictive model by combining the predictions of multiple weak learners, typically decision trees. The \"gradient\" in Gradient Boosting refers to the optimization process used to minimize the loss function, which measures the difference between the predicted values and the actual target values. Gradient Boosting iteratively improves the model by training new weak learners that focus on the errors made by the previous ones. It is a powerful technique known for its high predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a51818-ffe4-4b77-8fbd-4b9e42c94a51",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6954be3d-3967-4ad6-9f75-84165edd59e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.760000013044841\n",
      "R-squared: -7.411841362880978e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4.5, 6])\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_estimators = 100\n",
    "\n",
    "# Initialize predictions\n",
    "predictions = np.zeros(len(X))\n",
    "\n",
    "# Gradient Boosting\n",
    "for _ in range(n_estimators):\n",
    "    # Calculate residuals (negative gradient)\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Fit a weak learner (decision tree) to the residuals\n",
    "    weak_learner = np.mean(residuals)  # Simplest weak learner: mean of residuals\n",
    "    \n",
    "    # Update predictions\n",
    "    predictions += learning_rate * weak_learner\n",
    "    \n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r_squared = r2_score(y, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c707a-1f40-4408-b51e-378d50c23658",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a146cf-34e4-47d2-8619-b2618d8e2901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best Mean Squared Error: 0.25745473587856404\n",
      "Best R-squared: 0.8537189000689978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create the Gradient Boosting Regressor\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=gb_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_gb_reg = GradientBoostingRegressor(**best_params)\n",
    "best_gb_reg.fit(X.reshape(-1, 1), y)\n",
    "best_predictions = best_gb_reg.predict(X.reshape(-1, 1))\n",
    "best_mse = mean_squared_error(y, best_predictions)\n",
    "best_r_squared = r2_score(y, best_predictions)\n",
    "\n",
    "print(\"Best Mean Squared Error:\", best_mse)\n",
    "print(\"Best R-squared:\", best_r_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b199e71-c15e-4f31-90f3-f500953ac5d2",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3dc523-d56e-4b27-95e7-56db57c2cbca",
   "metadata": {},
   "source": [
    "A4. In Gradient Boosting, a weak learner is a simple, base model that performs slightly better than random guessing on a given task. Typically, decision trees with limited depth (shallow trees) are used as weak learners. Weak learners are often referred to as \"base learners\" because they serve as the building blocks for the ensemble. In each boosting iteration, a weak learner is trained to predict the errors (residuals) made by the ensemble of previously trained weak learners. The idea is that by combining the predictions of many weak learners, the ensemble can learn complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e744d-fd75-48d5-b78d-c456bbfca899",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3e181-d9f9-45c8-9504-0e5b1935e123",
   "metadata": {},
   "source": [
    "A5. The intuition behind Gradient Boosting is to sequentially train a series of weak learners, each one focusing on the errors made by the previous learners. This approach is analogous to a \"team of experts\" where each expert specializes in correcting specific mistakes made by the team. By combining the expertise of these experts (weak learners), the ensemble becomes a strong learner capable of accurate predictions. Gradient Boosting optimizes the model by minimizing the gradient of a loss function, effectively moving the predictions closer to the target values in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e2e1b-06bd-47ac-8d0b-f1573a8093fb",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018a854-478a-42f4-ba34-06e8cb419c07",
   "metadata": {},
   "source": [
    "A6. Gradient Boosting builds an ensemble of weak learners through an iterative process. Here's how it works:\n",
    "\n",
    "- Initialize the ensemble's predictions with zeros or a constant.\n",
    "- Compute the residuals (differences between the true target values and current predictions).\n",
    "- Fit a weak learner (e.g., decision tree) to the residuals, aiming to capture the errors.\n",
    "- Adjust the ensemble's predictions by adding a fraction of the weak learner's predictions (controlled by the learning rate).\n",
    "- Repeat steps 2-4 for a specified number of iterations or until convergence.\n",
    "- The final ensemble combines the predictions of all weak learners.\n",
    "\n",
    "In each iteration, the focus is on improving the predictions where the ensemble is performing poorly, gradually reducing prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a536c7-100b-4125-85cb-95390cfb1817",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e047c9-f8d9-4fe0-8dad-9ddd7a71f5d0",
   "metadata": {},
   "source": [
    "A7. The mathematical intuition of Gradient Boosting involves the following steps:\n",
    "\n",
    "- Define a loss function: Start by defining a differentiable loss function that measures the error between predicted values and actual target values. Common loss functions include mean squared error (MSE) for regression and logistic loss for classification.\n",
    "- Initialize predictions: Initialize the ensemble's predictions with a constant or zero.\n",
    "- Calculate residuals: Compute the residuals by subtracting the current predictions from the actual target values.\n",
    "- Train a weak learner: Fit a weak learner (e.g., decision tree) to the residuals. The goal is to capture the errors made by the ensemble so far.\n",
    "- Compute the negative gradient of the loss: Calculate the negative gradient of the loss function with respect to the residuals. This gradient provides the direction and magnitude of the correction needed for the predictions.\n",
    "- Update the ensemble's predictions: Adjust the ensemble's predictions by adding a fraction (learning rate) of the weak learner's predictions, scaled by the computed gradient. This step moves the predictions closer to the target values.\n",
    "- Repeat: Repeat steps 3-6 for a specified number of iterations or until convergence.\n",
    "- Final ensemble: The final ensemble combines the predictions of all weak learners, which collectively form a strong learner capable of accurate predictions.\n",
    "\n",
    "By minimizing the loss function using the negative gradient, Gradient Boosting iteratively improves the ensemble's predictions, making them progressively closer to the true target values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664601be-fe27-4a17-9887-ae3b691dc068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
